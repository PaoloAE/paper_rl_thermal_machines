{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Paper Plots from Data\n",
    "This notebook allows to produce plots in the styles of Fig. 3, 4 and 5 of the manuscript\n",
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.join('..','src'))\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "import sac\n",
    "import sac_tri\n",
    "import plotting\n",
    "import colorsys\n",
    "from pathlib import Path\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib.offsetbox import AnchoredOffsetbox, TextArea, HPacker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-level Engine (Fig. 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data\n",
    "We first visualize the full logged data. In all cells below, replace ```log_dir``` with the corresponding folder with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/main/2021_02_01-14_12_31_two_level_engine/\"\n",
    "plotting.plot_sac_logs(log_dir,is_tri=True,plot_to_file_line=None,actions_per_log=6000, suppress_show=False,\n",
    "                       save_plot=False,actions_ylim=None,actions_to_plot=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export final deterministic protocol\n",
    "We load the saved policy, test it on the environment, and export a file with the deterministc policy. The upper panel shows the running reward exponentially weighed with a very large gamma. This is to validate convergence, and to get a good estimate of the power. The second panel shows the deterministic policy over the last steps, and finally the average output power is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/main/2021_02_01-14_12_31_two_level_engine/\"\n",
    "\n",
    "#load the trained model\n",
    "loaded_train = sac_tri.SacTrain()\n",
    "loaded_train.load_train(log_dir, no_train=True)\n",
    "#evaluate the deterministic policy\n",
    "loaded_train.evaluate_current_policy(deterministic=True, steps=6000, gamma=0.9999,actions_to_plot=70,\n",
    "                                     save_policy_to_file_name=\"det_policy.txt\",actions_ylim=[0.3,1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce final plot (Fig. 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/main/2021_02_01-14_12_31_two_level_engine/\"\n",
    "det_policy_sublocation = \"saved_policies/det_policy.txt\"\n",
    "actions_to_plot_large = 10\n",
    "actions_to_plot_small = 30\n",
    "actions_per_log = 6000\n",
    "act_0 = 6000-2\n",
    "act_1 = 240000-2\n",
    "act_2 = 492000-2\n",
    "prot_linewidth=4.\n",
    "small_action_ylim = [0.3,1]\n",
    "large_action_ylim = [0.45,0.8]\n",
    "reward_ylabel= r\"$100\\times\\ev*{P_{[\\text{E}]}}_\\gamma$\"\n",
    "reward_plot_extra_args = ([0,500000], [0.93,0.93])\n",
    "reward_plot_extra_kwargs = dict(color='black',linewidth=0.8, dashes=(4,4))\n",
    "reward_legend_labels=[\"RL Cycle\", \"Exact Bound\"]\n",
    "action_legend_lines=[Line2D([0], [0], color='orange', linewidth=4), \n",
    "                    Line2D([0], [0], color='cornflowerblue', linewidth=4),\n",
    "                    Line2D([0], [0], color='limegreen', linewidth=4)]\n",
    "action_legend_text=[\"Hot\",\"Cold\",\"None\"]\n",
    "action_legend_location=[0.22, 1.]\n",
    "plot_file_name = \"two_level_engine.pdf\"\n",
    "\n",
    "plotting.sac_paper_plot(log_dir, det_policy_sublocation,act_0,act_1,act_2,is_tri=True,\n",
    "                        actions_to_plot_large=actions_to_plot_large, actions_to_plot_small=actions_to_plot_small,\n",
    "                        actions_per_log=actions_per_log,prot_linewidth=prot_linewidth, plot_file_name=plot_file_name,\n",
    "                        small_action_ylim=small_action_ylim, large_action_ylim=large_action_ylim,\n",
    "                        reward_ylabel=reward_ylabel,reward_plot_extra_args=reward_plot_extra_args,\n",
    "                       reward_plot_extra_kwargs=reward_plot_extra_kwargs,reward_legend_labels=reward_legend_labels,\n",
    "                       action_legend_lines=action_legend_lines,action_legend_text=action_legend_text,\n",
    "                       action_legend_location=action_legend_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superconducting Qubit Refrigerator (Fig. 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data\n",
    "We first visualize the full logged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/main/2021_02_05-16_13_38_superconducting_qubit_refrigerator/\"\n",
    "\n",
    "plotting.plot_sac_logs(log_dir, is_tri=False, plot_to_file_line=None,actions_per_log=6000, suppress_show=False,\n",
    "                       save_plot=False,actions_ylim=None,actions_to_plot=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export final deterministic protocol\n",
    "We load the saved policy, test it on the environment, and export a file with the deterministc policy. The upper panel shows the running reward exponentially weighed with a very large gamma. This is to validate convergence, and to get a good estimate of the power. The second panel shows the deterministic policy over the last steps, and finally the average output power is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/main/2021_02_05-16_13_38_superconducting_qubit_refrigerator/\"\n",
    "\n",
    "#load the trained model\n",
    "loaded_train = sac.SacTrain()\n",
    "loaded_train.load_train(log_dir, no_train=True)\n",
    "#evaluate the deterministic policy\n",
    "loaded_train.evaluate_current_policy(deterministic=True, steps=8001, gamma=0.9999,actions_to_plot=70,\n",
    "                                     save_policy_to_file_name=\"det_policy.txt\",actions_ylim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce final plot (Fig. 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/main/2021_02_05-16_13_38_superconducting_qubit_refrigerator/\"\n",
    "det_policy_sublocation = \"saved_policies/det_policy.txt\"\n",
    "actions_to_plot_large = 100\n",
    "actions_to_plot_small = 60\n",
    "actions_per_log = 6000\n",
    "act_0 = 6000-2\n",
    "act_1 = 180000-2\n",
    "act_2 = 490000-2\n",
    "prot_linewidth=3.\n",
    "small_action_ylim = [-0.05, 0.8]\n",
    "large_action_ylim = [-0.05,0.55]\n",
    "reward_ylabel= r\"$10^4\\times\\ev*{P_{[\\text{R}]}}_\\gamma$\"\n",
    "reward_plot_extra_args = ([0,500000], [2,2])\n",
    "reward_plot_extra_kwargs = dict(color='black',linewidth=0.8, dashes=(4,4))\n",
    "extra_cycles = [lambda x,a=2,omega=0.065,dt=0.982: 0.25*(1. + np.tanh(a*np.cos(omega*x*dt))/np.tanh(a)),\n",
    "               [0,100],\"black\"]\n",
    "extra_cycles_linewidth = 0.8\n",
    "reward_legend_labels=['RL Cycle', 'Trapezoidal Cycle']\n",
    "plot_file_name = \"qubit_refrigerator.pdf\"\n",
    "\n",
    "#get location of files\n",
    "running_reward_file, _, actions_file = \\\n",
    "                                plotting.sac_logs_file_location(log_dir, False, None,None,None)\n",
    "\n",
    "#font size\n",
    "matplotlib.rcParams.update({'font.size': 14, \"text.usetex\": True,\n",
    "                            'text.latex.preamble' : r'\\usepackage{amsmath}\\usepackage{physics}'})\n",
    "\n",
    "#create the axis (subplots)\n",
    "fig = plt.figure(constrained_layout=True, figsize=(6,5))\n",
    "gs = gridspec.GridSpec(3, 4, figure=fig, height_ratios = [1,0.7,1],width_ratios=[0.5,0.5,0.3,0.2])\n",
    "reward_ax = fig.add_subplot(gs[0, :])\n",
    "prot_0_ax = fig.add_subplot(gs[1, 0])\n",
    "prot_1_ax = fig.add_subplot(gs[1, 1],sharey=prot_0_ax)\n",
    "prot_2_ax = fig.add_subplot(gs[1, 2:],sharey=prot_0_ax)\n",
    "prot_final_ax = fig.add_subplot(gs[2, 0:3])\n",
    "coupling_ax = fig.add_subplot(gs[2, 3],sharey=prot_final_ax)\n",
    "plt.setp(prot_1_ax.get_yticklabels(), visible=False)\n",
    "plt.setp(prot_2_ax.get_yticklabels(), visible=False)          \n",
    "plt.setp(coupling_ax.get_yticklabels(), visible=False)          \n",
    "\n",
    "#set the reward axis\n",
    "plotting.plot_running_reward_on_axis(running_reward_file, reward_ax, plot_to_file_line = None, linewidth=2.7,\n",
    "                                    custom_color = \"black\", lines_to_mark = [plotting.nearest_int(act_0/actions_per_log),\n",
    "                                    plotting.nearest_int(act_1/actions_per_log),plotting.nearest_int(act_2/actions_per_log)],\n",
    "                                    custom_mark_color=\"black\",ylim=None,ylabel=reward_ylabel,\n",
    "                                    plot_extra_args=reward_plot_extra_args, plot_extra_kwargs=reward_plot_extra_kwargs,\n",
    "                                    legend_labels=reward_legend_labels)\n",
    "\n",
    "#set the three actions axis\n",
    "plotting.plot_actions_on_axis(actions_file, prot_0_ax, is_tri=False, actions_to_plot=actions_to_plot_small,\n",
    "                            actions_ylim=small_action_ylim,plot_to_file_line=act_0,constant_steps=True,\n",
    "                            linewidth = prot_linewidth,two_xticks=True)\n",
    "plotting.plot_actions_on_axis(actions_file, prot_1_ax, is_tri=False, actions_to_plot=actions_to_plot_small,\n",
    "                            plot_to_file_line=act_1,ylabel=\"\",\n",
    "                            constant_steps=True, linewidth = prot_linewidth,two_xticks=True)\n",
    "plotting.plot_actions_on_axis(actions_file, prot_2_ax, is_tri=False, actions_to_plot=actions_to_plot_small,\n",
    "                            plot_to_file_line=act_2, ylabel=\"\",\n",
    "                            constant_steps=True, linewidth = prot_linewidth, two_xticks=True)\n",
    "\n",
    "#set the final protocol axis\n",
    "plotting.plot_actions_on_axis(log_dir + det_policy_sublocation, prot_final_ax, False, actions_to_plot=actions_to_plot_large,\n",
    "                            actions_ylim=large_action_ylim,plot_to_file_line=None,constant_steps=True,\n",
    "                            k_notation=False, x_count_from_zero=True,linewidth = prot_linewidth,\n",
    "                            xlabel=\"$t[dt]$\",extra_cycles=extra_cycles, extra_cycles_linewidth=extra_cycles_linewidth)\n",
    "\n",
    "#coupling strength functions for panel d\n",
    "s = lambda de, b, w: g/2 * 1/(1 + q**2*(de/w - w/de)**2)*de/(np.exp(b*de) - 1)\n",
    "s_tot = lambda de, b, w: s(de,b,w)+s(-de,b,w)\n",
    "s_hot = lambda de: s_tot(de,bh,wh)\n",
    "s_cold = lambda de: s_tot(de,bc,wc)\n",
    "de = lambda u: 2*e0*np.sqrt(d**2 + u**2);\n",
    "\n",
    "#parameters used for this figure\n",
    "g = 1;q = 30;e0 = 1;d = 0.12;wh = 1.03;wc = 0.24;bh = 10/3;bc = 2*bh;ec = 0.24;\n",
    "\n",
    "#plot panel d\n",
    "u_vals = np.linspace(0,0.6,100)\n",
    "hot_coupling = s_hot(de(u_vals))\n",
    "cold_coupling = s_cold(de(u_vals))\n",
    "coupling_ax.plot(hot_coupling, u_vals, color=\"red\")\n",
    "coupling_ax.plot(cold_coupling, u_vals, color=\"blue\")\n",
    "coupling_ax.set_xlim([0.,0.55])\n",
    "\n",
    "#add the colored label to panel d\n",
    "xbox1 = TextArea(r\"$\\gamma^{(\\text{C})}_{u(t)},$\", textprops=dict(color=\"b\", ha='center',va='bottom'))\n",
    "xbox2 = TextArea(r\"$\\gamma^{(\\text{H})}_{u(t)}$\", textprops=dict(color=\"r\", ha='center',va='bottom'))\n",
    "xbox = HPacker(children=[xbox1, xbox2],align=\"bottom\", pad=0, sep=5)\n",
    "anchored_xbox = AnchoredOffsetbox(loc=3, child=xbox, pad=0., frameon=False, bbox_to_anchor=(0.2, -0.6), \n",
    "                                  bbox_transform=coupling_ax.transAxes, borderpad=0.)\n",
    "coupling_ax.add_artist(anchored_xbox)\n",
    "\n",
    "#add the (a) (b) (c) (d) labels\n",
    "reward_ax.text(-0.12,-0.38, r'\\textbf{(a)}', transform=reward_ax.transAxes )\n",
    "prot_0_ax.text(-0.4,-0.55, r'\\textbf{(b)}', transform=prot_0_ax.transAxes )\n",
    "prot_final_ax.text(-0.16,-0.43, r'\\textbf{(c)}', transform=prot_final_ax.transAxes )\n",
    "prot_final_ax.text(0.95,-0.43, r'\\textbf{(d)}', transform=prot_final_ax.transAxes )\n",
    "\n",
    "#save if necessary\n",
    "if plot_file_name is not None:\n",
    "    plot_folder = os.path.join(log_dir, plotting.PLOT_DIR_NAME)\n",
    "    Path(plot_folder).mkdir(parents=True, exist_ok=True)\n",
    "    plot_file_name = os.path.join(plot_folder, plot_file_name)\n",
    "    plt.savefig(plot_file_name)\n",
    "\n",
    "#show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Harmonic Oscillator Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data of left panel\n",
    "We first visualize the full logged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/main/2021_02_02-18_41_42_harmonic_engine/\"\n",
    "\n",
    "plotting.plot_sac_logs(log_dir,is_tri=True,plot_to_file_line=None,actions_per_log=6000, suppress_show=False,\n",
    "                       save_plot=False,actions_ylim=None,actions_to_plot=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export final deterministic protocol of left panel\n",
    "We load the saved policy, test it on the environment, and export a file with the deterministc policy. The upper panel shows the running reward exponentially weighed with a very large gamma. This is to validate convergence, and to get a good estimate of the power. The second panel shows the deterministic policy over the last steps, and finally the average output power is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/main/2021_02_02-18_41_42_harmonic_engine/\"\n",
    "\n",
    "#load the trained model\n",
    "loaded_train = sac_tri.SacTrain()\n",
    "loaded_train.load_train(log_dir, no_train=True)\n",
    "#evaluate the deterministic policy\n",
    "loaded_train.evaluate_current_policy(deterministic=True, steps=8000-2, gamma=0.9999,actions_to_plot=70,\n",
    "                                     save_policy_to_file_name=\"det_policy.txt\",actions_ylim=[0.3,1.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data of right panel\n",
    "We first visualize the full logged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/main/2021_02_03-09_42_46_harmonic_engine_larger_range/\"\n",
    "\n",
    "plotting.plot_sac_logs(log_dir,is_tri=True,plot_to_file_line=None,actions_per_log=6000, suppress_show=False,\n",
    "                       save_plot=False,actions_ylim=None,actions_to_plot=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export final deterministic protocol of right panel\n",
    "We load the saved policy, test it on the environment, and export a file with the deterministc policy. The upper panel shows the running reward exponentially weighed with a very large gamma. This is to validate convergence, and to get a good estimate of the power. The second panel shows the deterministic policy over the last steps, and finally the average output power is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/main/2021_02_03-09_42_46_harmonic_engine_larger_range/\"\n",
    "\n",
    "loaded_train = sac_tri.SacTrain()\n",
    "loaded_train.load_train(log_dir, no_train=True)\n",
    "loaded_train.evaluate_current_policy(deterministic=True, steps=8760-150, gamma=0.9999,actions_to_plot=200,\n",
    "                                     save_policy_to_file_name=\"det_policy.txt\",actions_ylim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce final plot (Fig. 5)\n",
    "It will be located in the Jupyter Notebook directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_0_dir = \"../paper_plot_data/main/2021_02_02-18_41_42_harmonic_engine/\"\n",
    "log_1_dir = \"../paper_plot_data/main/2021_02_03-09_42_46_harmonic_engine_larger_range/\"\n",
    "det_policy_sublocation = \"saved_policies/det_policy.txt\"\n",
    "actions_to_plot_0 = 66\n",
    "actions_to_plot_1 = 98\n",
    "actions_per_log = 6000\n",
    "custom_colors=[\"orange\",\"cornflowerblue\",\"limegreen\"]\n",
    "prot_linewidth=4.\n",
    "reward_linewidth = None\n",
    "action_ylim = [0.25,1.6]\n",
    "reward_ylim = [-2.1,2.5]\n",
    "reward_ylabel= r\"$10\\times\\ev*{P_{[\\text{E}]}}_\\gamma$\"\n",
    "reward_plot_extra_args_0 = ([0,250000], [1.31,1.31])\n",
    "reward_plot_extra_kwargs_0 = dict(color='black',linewidth=0.8, dashes=(4,4))\n",
    "reward_plot_extra_args_1 = ([0,600000], [1.31,1.31])\n",
    "reward_plot_extra_kwargs_1 = dict(color='black',linewidth=0.8, dashes=(4,4))\n",
    "extra_cycles_0 = plotting.produce_otto_cycle(u_min=0.5,u_max=1.,t1=2.9,t2=1.79,t3=3.2,t4=1.67,dt=0.2,t_range=[0,66*0.2])\n",
    "extra_cycles_linewidth_0 = 1.5\n",
    "extra_cycles_1 = plotting.produce_otto_cycle(u_min=0.5,u_max=1.,t1=2.9,t2=1.79,t3=3.2,t4=1.67,dt=0.2,t_range=[0,98*0.2])\n",
    "extra_cycles_linewidth_1 = 1.5\n",
    "cycle_legend_lines=[Line2D([0], [0], color='orange', linewidth=4), \n",
    "                    Line2D([0], [0], color='cornflowerblue', linewidth=4),\n",
    "                    Line2D([0], [0], color='limegreen', linewidth=4)]\n",
    "cycle_legend_text=[\"Hot\",\"Cold\",\"None\"]\n",
    "cycle_legend_location=[-0.63,0.05]\n",
    "plot_file_location = \"harmonic_engine.pdf\"\n",
    "\n",
    "\n",
    "#get location of files\n",
    "running_reward_file_0, running_loss_file_0, actions_file_0 = \\\n",
    "                                plotting.sac_logs_file_location(log_0_dir,True,None,None,None)\n",
    "running_reward_file_1, running_loss_file_1, actions_file_1 = \\\n",
    "                                plotting.sac_logs_file_location(log_1_dir,True,None,None,None)\n",
    "\n",
    "#font size\n",
    "matplotlib.rcParams.update({'font.size': 14, \"text.usetex\": True,\n",
    "                            'text.latex.preamble' : r'\\usepackage{amsmath}\\usepackage{physics}'})\n",
    "\n",
    "#create the axis (subplots)\n",
    "fig = plt.figure(constrained_layout=True, figsize=(6,3.5))\n",
    "gs = gridspec.GridSpec(2, 2, figure=fig, height_ratios = [1,1])\n",
    "reward_0_ax = fig.add_subplot(gs[0, 0])\n",
    "reward_1_ax = fig.add_subplot(gs[0, 1],sharey=reward_0_ax)\n",
    "prot_final_0_ax = fig.add_subplot(gs[1, 0])\n",
    "prot_final_1_ax = fig.add_subplot(gs[1, 1],sharey=prot_final_0_ax)\n",
    "plt.setp(reward_1_ax.get_yticklabels(), visible=False)          \n",
    "plt.setp(prot_final_1_ax.get_yticklabels(), visible=False)          \n",
    "\n",
    "\n",
    "#set the reward axis\n",
    "plotting.plot_running_reward_on_axis(running_reward_file_0, reward_0_ax, plot_to_file_line = None, \n",
    "                                     linewidth=reward_linewidth,custom_color = \"black\",ylim=reward_ylim,\n",
    "                                     ylabel=reward_ylabel,plot_extra_args=reward_plot_extra_args_0, \n",
    "                                     plot_extra_kwargs=reward_plot_extra_kwargs_0,yticks=[-2.,0.,2.5],\n",
    "                                     legend_labels=['RL Cycle', 'Otto Cycle'])\n",
    "\n",
    "plotting.plot_running_reward_on_axis(running_reward_file_1, reward_1_ax, plot_to_file_line = None, \n",
    "                                     linewidth=reward_linewidth,custom_color = \"black\",ylim=reward_ylim,\n",
    "                                     ylabel=\"\",plot_extra_args=reward_plot_extra_args_1, \n",
    "                                     plot_extra_kwargs=reward_plot_extra_kwargs_1)\n",
    "\n",
    "\n",
    "#set the final protocol axis\n",
    "plotting.plot_actions_on_axis(log_0_dir + det_policy_sublocation, prot_final_0_ax, is_tri=True,\n",
    "                            actions_to_plot=actions_to_plot_0, actions_ylim=action_ylim,plot_to_file_line=None,\n",
    "                            custom_colors=custom_colors, constant_steps=True, k_notation=False, x_count_from_zero=True,\n",
    "                            linewidth = prot_linewidth,extra_cycles=extra_cycles_0,\n",
    "                            extra_cycles_linewidth=extra_cycles_linewidth_0,xlabel=\"$t[dt]$\",\n",
    "                            legend_lines=cycle_legend_lines,legend_text=cycle_legend_text,\n",
    "                            legend_location=cycle_legend_location,legend_cols=1)\n",
    "plotting.plot_actions_on_axis(log_1_dir + det_policy_sublocation, prot_final_1_ax, is_tri=True,\n",
    "                            actions_to_plot=actions_to_plot_1, actions_ylim=action_ylim,plot_to_file_line=None,\n",
    "                            custom_colors=custom_colors, constant_steps=True, k_notation=False, x_count_from_zero=True,\n",
    "                            linewidth = prot_linewidth,extra_cycles=extra_cycles_1,\n",
    "                            extra_cycles_linewidth=extra_cycles_linewidth_1,xlabel=\"$t[dt]$\",ylabel=\"\")\n",
    "\n",
    "#add the (a) (b) (c) (d) labels\n",
    "reward_0_ax.text(0.,-0.46, r'\\textbf{(a)}', transform=reward_0_ax.transAxes )\n",
    "reward_1_ax.text(0.,-0.46, r'\\textbf{(b)}', transform=reward_1_ax.transAxes )\n",
    "prot_final_0_ax.text(0.,-0.46, r'\\textbf{(c)}', transform=prot_final_0_ax.transAxes )\n",
    "prot_final_1_ax.text(0.,-0.46, r'\\textbf{(d)}', transform=prot_final_1_ax.transAxes )\n",
    "\n",
    "#save file\n",
    "plt.savefig(plot_file_location)\n",
    "#show\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
