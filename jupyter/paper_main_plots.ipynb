{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Paper Plots from Data\n",
    "This notebook allows to produce plots in the styles of Fig. 3, 4 and 5 of the manuscript\n",
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.join('..','src'))\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "import sac\n",
    "import sac_tri\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-level Engine (Fig. 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data\n",
    "We first visualize the full logged data. In all cells below, replace ```log_dir``` with the corresponding folder with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/2021_02_01-14_12_31_two_level_engine/\"\n",
    "plotting.plot_sac_logs(log_dir,is_tri=True,plot_to_file_line=None,actions_per_log=6000, suppress_show=False,\n",
    "                       save_plot=False,actions_ylim=None,actions_to_plot=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export final deterministic protocol\n",
    "We load the saved policy, test it on the environment, and export a file with the deterministc policy. The upper panel shows the running reward exponentially weighed with a very large gamma. This is to validate convergence, and to get a good estimate of the power. The second panel shows the deterministic policy over the last steps, and finally the average output power is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/2021_02_01-14_12_31_two_level_engine/\"\n",
    "\n",
    "#load the trained model\n",
    "loaded_train = sac_tri.SacTrain()\n",
    "loaded_train.load_train(log_dir, no_train=True)\n",
    "#evaluate the deterministic policy\n",
    "loaded_train.evaluate_current_policy(deterministic=True, steps=6000, gamma=0.9999,actions_to_plot=70,\n",
    "                                     save_policy_to_file_name=\"det_policy.txt\",actions_ylim=[0.3,1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce final plot (Fig. 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/2021_02_01-14_12_31_two_level_engine/\"\n",
    "det_policy_sublocation = \"saved_policies/det_policy.txt\"\n",
    "actions_to_plot_large = 10\n",
    "actions_to_plot_small = 30\n",
    "actions_per_log = 6000\n",
    "act_0 = 6000-2\n",
    "act_1 = 240000-2\n",
    "act_2 = 492000-2\n",
    "prot_linewidth=4.\n",
    "small_action_ylim = [0.3,1]\n",
    "large_action_ylim = [0.45,0.8]\n",
    "reward_ylabel= r\"$100\\times\\ev*{P_{[\\text{E}]}}_\\gamma$\"\n",
    "reward_plot_extra_args = ([0,500000], [0.93,0.93])\n",
    "reward_plot_extra_kwargs = dict(color='black',linewidth=0.8, dashes=(4,4))\n",
    "reward_legend_labels=[\"RL Cycle\", \"Exact Bound\"]\n",
    "action_legend_lines=[Line2D([0], [0], color='orange', linewidth=4), \n",
    "                    Line2D([0], [0], color='cornflowerblue', linewidth=4),\n",
    "                    Line2D([0], [0], color='limegreen', linewidth=4)]\n",
    "action_legend_text=[\"Hot\",\"Cold\",\"None\"]\n",
    "action_legend_location=[0.22, 1.]\n",
    "plot_file_name = \"two_level_engine.pdf\"\n",
    "\n",
    "plotting.sac_paper_plot(log_dir, det_policy_sublocation,act_0,act_1,act_2,is_tri=True,\n",
    "                        actions_to_plot_large=actions_to_plot_large, actions_to_plot_small=actions_to_plot_small,\n",
    "                        actions_per_log=actions_per_log,prot_linewidth=prot_linewidth, plot_file_name=plot_file_name,\n",
    "                        small_action_ylim=small_action_ylim, large_action_ylim=large_action_ylim,\n",
    "                        reward_ylabel=reward_ylabel,reward_plot_extra_args=reward_plot_extra_args,\n",
    "                       reward_plot_extra_kwargs=reward_plot_extra_kwargs,reward_legend_labels=reward_legend_labels,\n",
    "                       action_legend_lines=action_legend_lines,action_legend_text=action_legend_text,\n",
    "                       action_legend_location=action_legend_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superconducting Qubit Refrigerator (Fig. 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data\n",
    "We first visualize the full logged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/2021_02_05-16_13_38_superconducting_qubit_refrigerator/\"\n",
    "\n",
    "plotting.plot_sac_logs(log_dir, is_tri=False, plot_to_file_line=None,actions_per_log=6000, suppress_show=False,\n",
    "                       save_plot=False,actions_ylim=None,actions_to_plot=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export final deterministic protocol\n",
    "We load the saved policy, test it on the environment, and export a file with the deterministc policy. The upper panel shows the running reward exponentially weighed with a very large gamma. This is to validate convergence, and to get a good estimate of the power. The second panel shows the deterministic policy over the last steps, and finally the average output power is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/2021_02_05-16_13_38_superconducting_qubit_refrigerator/\"\n",
    "\n",
    "#load the trained model\n",
    "loaded_train = sac.SacTrain()\n",
    "loaded_train.load_train(log_dir, no_train=True)\n",
    "#evaluate the deterministic policy\n",
    "loaded_train.evaluate_current_policy(deterministic=True, steps=8001, gamma=0.9999,actions_to_plot=70,\n",
    "                                     save_policy_to_file_name=\"det_policy.txt\",actions_ylim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce final plot (Fig. 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/2021_02_05-16_13_38_superconducting_qubit_refrigerator/\"\n",
    "det_policy_sublocation = \"saved_policies/det_policy.txt\"\n",
    "actions_to_plot_large = 100\n",
    "actions_to_plot_small = 60\n",
    "actions_per_log = 6000\n",
    "act_0 = 6000-2\n",
    "act_1 = 180000-2\n",
    "act_2 = 490000-2\n",
    "prot_linewidth=3.\n",
    "small_action_ylim = [-0.05, 0.8]\n",
    "large_action_ylim = [-0.05,0.55]\n",
    "reward_ylabel= r\"$10^4\\times\\ev*{P_{[\\text{R}]}}_\\gamma$\"\n",
    "reward_plot_extra_args = ([0,500000], [2,2])\n",
    "reward_plot_extra_kwargs = dict(color='black',linewidth=0.8, dashes=(4,4))\n",
    "extra_cycles = [lambda x,a=2,omega=0.065,dt=0.982: 0.25*(1. + np.tanh(a*np.cos(omega*x*dt))/np.tanh(a)),\n",
    "               [0,100],\"black\"]\n",
    "extra_cycles_linewidth = 0.8\n",
    "reward_legend_labels=['RL Cycle', 'Trapezoidal Cycle']\n",
    "plot_file_name = \"qubit_refrigerator.pdf\"\n",
    "\n",
    "\n",
    "plotting.sac_paper_plot(log_dir, det_policy_sublocation,act_0,act_1,act_2,is_tri=False,\n",
    "                        actions_to_plot_large=actions_to_plot_large, actions_to_plot_small=actions_to_plot_small,\n",
    "                        actions_per_log=actions_per_log,prot_linewidth=prot_linewidth,plot_file_name=plot_file_name,\n",
    "                        small_action_ylim=small_action_ylim, large_action_ylim=large_action_ylim,\n",
    "                        reward_plot_extra_args=reward_plot_extra_args, reward_plot_extra_kwargs=reward_plot_extra_kwargs,\n",
    "                        reward_ylabel=reward_ylabel,extra_cycles=extra_cycles,extra_cycles_linewidth=extra_cycles_linewidth,\n",
    "                        reward_legend_labels=reward_legend_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Harmonic Oscillator Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data of left panel\n",
    "We first visualize the full logged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/2021_02_02-18_41_42_harmonic_engine/\"\n",
    "\n",
    "plotting.plot_sac_logs(log_dir,is_tri=True,plot_to_file_line=None,actions_per_log=6000, suppress_show=False,\n",
    "                       save_plot=False,actions_ylim=None,actions_to_plot=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export final deterministic protocol of left panel\n",
    "We load the saved policy, test it on the environment, and export a file with the deterministc policy. The upper panel shows the running reward exponentially weighed with a very large gamma. This is to validate convergence, and to get a good estimate of the power. The second panel shows the deterministic policy over the last steps, and finally the average output power is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/2021_02_02-18_41_42_harmonic_engine/\"\n",
    "\n",
    "#load the trained model\n",
    "loaded_train = sac_tri.SacTrain()\n",
    "loaded_train.load_train(log_dir, no_train=True)\n",
    "#evaluate the deterministic policy\n",
    "loaded_train.evaluate_current_policy(deterministic=True, steps=8000-2, gamma=0.9999,actions_to_plot=70,\n",
    "                                     save_policy_to_file_name=\"det_policy.txt\",actions_ylim=[0.3,1.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data of right panel\n",
    "We first visualize the full logged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/2021_02_03-09_42_46_harmonic_engine_larger_range/\"\n",
    "\n",
    "plotting.plot_sac_logs(log_dir,is_tri=True,plot_to_file_line=None,actions_per_log=6000, suppress_show=False,\n",
    "                       save_plot=False,actions_ylim=None,actions_to_plot=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export final deterministic protocol of right panel\n",
    "We load the saved policy, test it on the environment, and export a file with the deterministc policy. The upper panel shows the running reward exponentially weighed with a very large gamma. This is to validate convergence, and to get a good estimate of the power. The second panel shows the deterministic policy over the last steps, and finally the average output power is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../paper_plot_data/2021_02_03-09_42_46_harmonic_engine_larger_range/\"\n",
    "\n",
    "loaded_train = sac_tri.SacTrain()\n",
    "loaded_train.load_train(log_dir, no_train=True)\n",
    "loaded_train.evaluate_current_policy(deterministic=True, steps=8760-150, gamma=0.9999,actions_to_plot=200,\n",
    "                                     save_policy_to_file_name=\"det_policy.txt\",actions_ylim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce final plot (Fig. 5)\n",
    "It will be located in the Jupyter Notebook directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_0_dir = \"../paper_plot_data/2021_02_02-18_41_42_harmonic_engine/\"\n",
    "log_1_dir = \"../paper_plot_data/2021_02_03-09_42_46_harmonic_engine_larger_range/\"\n",
    "det_policy_sublocation = \"saved_policies/det_policy.txt\"\n",
    "actions_to_plot_0 = 66\n",
    "actions_to_plot_1 = 98\n",
    "actions_per_log = 6000\n",
    "custom_colors=[\"orange\",\"cornflowerblue\",\"limegreen\"]\n",
    "prot_linewidth=4.\n",
    "reward_linewidth = None\n",
    "action_ylim = [0.25,1.6]\n",
    "reward_ylim = [-2.1,2.5]\n",
    "reward_ylabel= r\"$10\\times\\ev*{P_{[\\text{E}]}}_\\gamma$\"\n",
    "reward_plot_extra_args_0 = ([0,250000], [1.31,1.31])\n",
    "reward_plot_extra_kwargs_0 = dict(color='black',linewidth=0.8, dashes=(4,4))\n",
    "reward_plot_extra_args_1 = ([0,600000], [1.31,1.31])\n",
    "reward_plot_extra_kwargs_1 = dict(color='black',linewidth=0.8, dashes=(4,4))\n",
    "extra_cycles_0 = plotting.produce_otto_cycle(u_min=0.5,u_max=1.,t1=2.9,t2=1.79,t3=3.2,t4=1.67,dt=0.2,t_range=[0,66*0.2])\n",
    "extra_cycles_linewidth_0 = 1.5\n",
    "extra_cycles_1 = plotting.produce_otto_cycle(u_min=0.5,u_max=1.,t1=2.9,t2=1.79,t3=3.2,t4=1.67,dt=0.2,t_range=[0,98*0.2])\n",
    "extra_cycles_linewidth_1 = 1.5\n",
    "cycle_legend_lines=[Line2D([0], [0], color='orange', linewidth=4), \n",
    "                    Line2D([0], [0], color='cornflowerblue', linewidth=4),\n",
    "                    Line2D([0], [0], color='limegreen', linewidth=4)]\n",
    "cycle_legend_text=[\"Hot\",\"Cold\",\"None\"]\n",
    "cycle_legend_location=[-0.63,0.05]\n",
    "plot_file_location = \"harmonic_engine.pdf\"\n",
    "\n",
    "\n",
    "#get location of files\n",
    "running_reward_file_0, running_loss_file_0, actions_file_0 = \\\n",
    "                                plotting.sac_logs_file_location(log_0_dir,True,None,None,None)\n",
    "running_reward_file_1, running_loss_file_1, actions_file_1 = \\\n",
    "                                plotting.sac_logs_file_location(log_1_dir,True,None,None,None)\n",
    "\n",
    "#font size\n",
    "matplotlib.rcParams.update({'font.size': 14, \"text.usetex\": True,\n",
    "                            'text.latex.preamble' : r'\\usepackage{amsmath}\\usepackage{physics}'})\n",
    "\n",
    "#create the axis (subplots)\n",
    "fig = plt.figure(constrained_layout=True, figsize=(6,3.5))\n",
    "gs = gridspec.GridSpec(2, 2, figure=fig, height_ratios = [1,1])\n",
    "reward_0_ax = fig.add_subplot(gs[0, 0])\n",
    "reward_1_ax = fig.add_subplot(gs[0, 1],sharey=reward_0_ax)\n",
    "prot_final_0_ax = fig.add_subplot(gs[1, 0])\n",
    "prot_final_1_ax = fig.add_subplot(gs[1, 1],sharey=prot_final_0_ax)\n",
    "plt.setp(reward_1_ax.get_yticklabels(), visible=False)          \n",
    "plt.setp(prot_final_1_ax.get_yticklabels(), visible=False)          \n",
    "\n",
    "\n",
    "#set the reward axis\n",
    "plotting.plot_running_reward_on_axis(running_reward_file_0, reward_0_ax, plot_to_file_line = None, \n",
    "                                     linewidth=reward_linewidth,custom_color = \"black\",ylim=reward_ylim,\n",
    "                                     ylabel=reward_ylabel,plot_extra_args=reward_plot_extra_args_0, \n",
    "                                     plot_extra_kwargs=reward_plot_extra_kwargs_0,yticks=[-2.,0.,2.5],\n",
    "                                     legend_labels=['RL Cycle', 'Otto Cycle'])\n",
    "\n",
    "plotting.plot_running_reward_on_axis(running_reward_file_1, reward_1_ax, plot_to_file_line = None, \n",
    "                                     linewidth=reward_linewidth,custom_color = \"black\",ylim=reward_ylim,\n",
    "                                     ylabel=\"\",plot_extra_args=reward_plot_extra_args_1, \n",
    "                                     plot_extra_kwargs=reward_plot_extra_kwargs_1)\n",
    "\n",
    "\n",
    "#set the final protocol axis\n",
    "plotting.plot_actions_on_axis(log_0_dir + det_policy_sublocation, prot_final_0_ax, is_tri=True,\n",
    "                            actions_to_plot=actions_to_plot_0, actions_ylim=action_ylim,plot_to_file_line=None,\n",
    "                            custom_colors=custom_colors, constant_steps=True, k_notation=False, x_count_from_zero=True,\n",
    "                            linewidth = prot_linewidth,extra_cycles=extra_cycles_0,\n",
    "                            extra_cycles_linewidth=extra_cycles_linewidth_0,xlabel=\"$t[dt]$\",\n",
    "                            legend_lines=cycle_legend_lines,legend_text=cycle_legend_text,\n",
    "                            legend_location=cycle_legend_location,legend_cols=1)\n",
    "plotting.plot_actions_on_axis(log_1_dir + det_policy_sublocation, prot_final_1_ax, is_tri=True,\n",
    "                            actions_to_plot=actions_to_plot_1, actions_ylim=action_ylim,plot_to_file_line=None,\n",
    "                            custom_colors=custom_colors, constant_steps=True, k_notation=False, x_count_from_zero=True,\n",
    "                            linewidth = prot_linewidth,extra_cycles=extra_cycles_1,\n",
    "                            extra_cycles_linewidth=extra_cycles_linewidth_1,xlabel=\"$t[dt]$\",ylabel=\"\")\n",
    "\n",
    "#add the (a) (b) (c) (d) labels\n",
    "reward_0_ax.text(0.,-0.46, r'(a)', transform=reward_0_ax.transAxes )\n",
    "reward_1_ax.text(0.,-0.46, r'(b)', transform=reward_1_ax.transAxes )\n",
    "prot_final_0_ax.text(0.,-0.46, r'(c)', transform=prot_final_0_ax.transAxes )\n",
    "prot_final_1_ax.text(0.,-0.46, r'(d)', transform=prot_final_1_ax.transAxes )\n",
    "\n",
    "#save file\n",
    "plt.savefig(plot_file_location)\n",
    "#show\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
