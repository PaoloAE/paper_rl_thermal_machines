{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superconducting Qubit Refrigerator\n",
    "Optimize the cooling power of a refrigerator based on a superconducting qubit (see section IVb of manuscript or Refs. [1](https://doi.org/10.1103/PhysRevB.94.184503), [2](https://doi.org/10.1103/PhysRevB.100.085405) or [3](https://doi.org/10.1103/PhysRevB.100.035407)). The Hamiltonian of the system is:\n",
    "\\begin{equation}\n",
    "\t\\hat{H}[u(t)] = - E_0\\left[\\Delta \\hat{\\sigma}_x + u(t)\\hat{\\sigma}_z  \\right],\n",
    "\t\\label{eq:h_fridge}\n",
    "\\end{equation}\n",
    "where $\\hat{\\sigma}_x$ and $\\hat{\\sigma}_z$ are Pauli matrices, $E_0$ is a fixed energy scale, $\\Delta$ characterizes the minimum gap of the system, and $u(t)$ is our single continuous control parameter. In this setup the coupling to the bath is fixed, so we do not have the discrete action of choosing the bath.\n",
    "The coupling to the baths is described using the Lindblad master equation [see Eq. (9) of the manuscript]. The Lindblad operators and corresponding rates are gived by\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\t\\hat{A}^{(\\alpha)}_{+,u(t)} &= -i\\rvert e_{u(t)}\\rangle \n",
    "    \\langle g_{u(t)} \\rvert, &\n",
    "\t\\hat{A}^{(\\alpha)}_{-,u(t)} &= +i\\rvert g_{u(t)}\\rangle \\langle e_{u(t)}\\rvert,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $\\rvert g_{u(t)}\\rangle$ and $\\rvert e_{u(t)}\\rangle$ are, respectively, the instantaneous ground state and excited state of the qubit. The corresponding rates are given by $\\gamma^{(\\alpha)}_{\\pm,u(t)} = S_{\\alpha}[\\pm\\Delta \\epsilon_{u(t)}] $, where $\\Delta \\epsilon_{u(t)}$ is the instantaneous energy gap of the system, and\n",
    "\\begin{equation}\n",
    "\tS_\\alpha(\\Delta \\epsilon)= \\frac{g_{\\alpha}}{2} \\frac{1}{1+Q_\\alpha^2( \\Delta\\epsilon/\\omega_\\alpha - \\omega_\\alpha/\\Delta \\epsilon )^2 } \\frac{\\Delta \\epsilon}{e^{\\beta_\\alpha\\Delta\\epsilon}-1}\n",
    "\\end{equation}\n",
    "is the noise power spectrum of bath $\\alpha$. Here $\\omega_\\alpha$, $Q_\\alpha$ and $g_\\alpha$ are the base resonance frequency, quality factor and coupling strength of the resonant circuit acting as bath $\\alpha=\\text{H},\\text{C}$, and $\\beta_\\alpha$ is the inverse temperature of bath $\\alpha$.\n",
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..','src'))\n",
    "import sac\n",
    "import sac_envs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup new Training\n",
    "The following codes initiates a new training session. All training logs, parameters and saved states will be stored under the ```data``` folder, within a folder with the current date and time. \n",
    "- ```env_params``` is a dictionary with the environment parameters.\n",
    "- ```training_hyperparams``` is a dictionary with training hyperparameters.\n",
    "- ```log_info``` is a dictionary that specifices which quantities to log.\n",
    "\n",
    "The parameters below were used to produce Fig. 4 of the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e0 = 1.\n",
    "delta = 0.12\n",
    "omega = 0.05 \n",
    "dt = 2.*np.pi /omega / 128. \n",
    "env_params = {\n",
    "    \"g0\": 1.,                             #g of bath 0\n",
    "    \"g1\": 1.,                             #g of bath 1\n",
    "    \"b0\": 1/0.3,                          #inverse temperature \\beta of bath 0\n",
    "    \"b1\": 1/0.15,                         #inverse temperature \\beta of bath 1\n",
    "    \"q0\": 30.,                            #quality factor of bath 0\n",
    "    \"q1\": 30.,                            #quality factor of bath 1\n",
    "    \"e0\": e0,                             #E_0\n",
    "    \"delta\": delta,                       #\\Delta\n",
    "    \"w0\": 2.*e0*np.sqrt(delta**2 + 0.25), #resonance frequency of bath 0\n",
    "    \"w1\": 2.*e0*delta,                    #resonance frequency of bath 1\n",
    "    \"min_u\": 0.,                          #minimum value of action u\n",
    "    \"max_u\": 0.75,                        #maximum value of action u\n",
    "    \"dt\": dt,                             #timestep \\Delta t\n",
    "    \"reward_extra_coeff\": 1.*10**4        #the reward is multiplied by this factor\n",
    "} \n",
    "\n",
    "training_hyperparams = {\n",
    "    \"BATCH_SIZE\": 256,                    #batch size\n",
    "    \"LR\": 0.001,                          #learning rate\n",
    "    \"ALPHA_START\": 50,                    #initial value of SAC temperature\n",
    "    \"ALPHA_END\": 0.,                      #final value of SAC temperature\n",
    "    \"ALPHA_DECAY\": 48000,                 #exponential decay of SAC temperature\n",
    "    \"REPLAY_MEMORY_SIZE\": 192000,         #size of replay buffer\n",
    "    \"POLYAK\": 0.995,                      #polyak coefficient\n",
    "    \"LOG_STEPS\": 6000,                    #save logs and display training every number of steps\n",
    "    \"GAMMA\": 0.995,                       #RL discount factor\n",
    "    \"HIDDEN_SIZES\": (256,256),            #size of hidden layers\n",
    "    \"SAVE_STATE_STEPS\": 80000,            #saves complete state of trainig every number of steps\n",
    "    \"INITIAL_RANDOM_STEPS\": 5000,         #number of initial uniformly random steps\n",
    "    \"UPDATE_AFTER\": 1000,                 #start minimizing loss function after initial steps\n",
    "    \"UPDATE_EVERY\": 50,                   #performs this many updates every this many steps\n",
    "    \"USE_CUDA\": False                     #use cuda for computation \n",
    "}\n",
    "\n",
    "log_info = {\n",
    "    \"log_running_reward\": True,           #log running reward \n",
    "    \"log_running_loss\": True,             #log running loss\n",
    "    \"log_actions\": True,                  #log chosen actions\n",
    "    \"extra_str\": \"_superconducting_qubit_refrigerator\" #extra string to append to training folder\n",
    "}\n",
    "\n",
    "\n",
    "train = sac.SacTrain()\n",
    "train.initialize_new_train(sac_envs.SuperconductingQubitRefrigerator,\n",
    "                           env_params, training_hyperparams, log_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "Perform a given number of training steps. It can be run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train(500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the State\n",
    "The full state of the training session is saved every ```SAVE_STATE_STEPS``` steps. Run this command if you wish to manually save the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.save_full_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Training\n",
    "Any training session that was saved can be loaded specifying the training session folder in ```log_dir```. This will produce a new folder for logging with the current date-time. The following loads the latest save in a folder named ```\"2021_04_14-10_51_14_superconducting_qubit_refrigerator\"```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../data/2021_04_14-10_51_14_superconducting_qubit_refrigerator/\"\n",
    "train = sac.SacTrain()\n",
    "train.load_train(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "Perform a given number of training steps. It can be run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually changing the learning rate\n",
    "The following is NOT RECOMMENDED, since the change is not logged. However, it is possible to change the learning rate during training. The following changes the learning rate to 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in train.pi_optimizer.param_groups:\n",
    "    g['lr'] = 0.0001\n",
    "for g in train.q_optimizer.param_groups:\n",
    "    g['lr'] = 0.0001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
